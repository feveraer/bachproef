%%=============================================================================
%% Onderzoek
%%=============================================================================

\chapter{Praktische uitwerking}
\label{ch:onderzoek}

\section{Methodologie}
\label{sec:methodologie}

\subsection{Opzetten microservices}
\label{sec:opzetten}

De opstelling kent de volgende microservices:
\begin{itemize}
\item eureka: service discovery, opdat de microservices intern met elkaar kunnen communiceren
\item zipkin: Zipkin server voor tracing
\item ms-web: Spring Boot app die root microservice voorstelt
\item ms-service-n: Spring Boot app voor bijkomende microservices (n staat voor een cijfer van 1 t.e.m. 27)
\end{itemize}

Dit onderzoekt bekijkt drie verschillende opstellingen, geonderscheid op complexiteit, om de tracing performantie na te gaan. De setup met lage complexiteit kent in totaal 4 microservices en 2 niveau's (zie figuur \ref{fig:lc_dependencies}).
\begin{figure}[h]
\caption{Lage complexiteit setup}
\includegraphics{ms_lc_dependencies}
\label{fig:lc_dependencies}
\end{figure}
De setup met gemiddelde complexiteit kent 11 microservices en 3 niveau's (zie figuur \ref{fig:mc_dependencies}).
\begin{figure}[h]
\caption{Gemiddelde complexiteit setup}
\includegraphics[scale=0.5]{ms_dependencies}
\label{fig:mc_dependencies}
\end{figure}
De setup met hoge complexiteit kent 28 microservices en 4 niveau's (zie figuur \ref{fig:hc_dependencies}). Een request naar ms-web zal dus, afhankelijk van de setup, meerdere interne requests maken naar andere microservices. Er wordt per opstelling onderzocht wat voor performantiegevolgen tracing met zich meedraagt.
\begin{figure}[h]
\caption{Hoge complexiteit setup}
\includegraphics[scale=0.5]{ms_hc_dependencies}
\label{fig:hc_dependencies}
\end{figure}

De Spring Cloud Sleuth bibliotheek wordt toegevoegd aan de ms-web en ms-service-n microservices, zodat deze services tracing info doorsturen naar de zipkin server.

Elke microservice krijgt zijn eigen Docker container, die allemaal tegelijk worden opgestart met Docker Compose~\autocite{DockerCompose2015}, dankzij het toevoegen van een docker-compose.yml script, die alle service definities bevat.

De microservices communiceren asynchroon met elkaar, zodat de ene service niet hoeft te wachten op de andere. Dit verbetert de responstijd. Vergelijk figuur \ref{fig:sync_rest} met figuur \ref{fig:async_rest}.

\begin{figure}
\caption{Synchrone REST calls tussen microservices, gevisualiseerd in Zipkin}
\centering
\includegraphics[width=1\textwidth]{sync_traces_zipkin}
\label{fig:sync_rest}
\end{figure}

\begin{figure}
\caption{Asynchrone REST calls tussen microservices, gevisualiseerd in Zipkin}
\centering
\includegraphics[width=1\textwidth]{async_traces_zipkin}
\label{fig:async_rest}
\end{figure} 

Om de asynchrone communicatie tussen services mogelijk te maken wordt gebruik gemaakt van \texttt{ListenableFuture} van guava (Google Core libraries voor Java) in combinatie met \texttt{TraceCallable} van Sleuth. \texttt{TraceCallable} is een wrapper van Sleuth voor een Java \texttt{Callable} die tracing informatie toevoegt. Een gewone \texttt{Callable} voert een taak uit op een andere thread en geeft een resultaat terug.

\begin{lstlisting}[language=Java, basicstyle=\ttfamily\scriptsize, caption=Asynchrone REST communicatie]
@RestController
@RequestMapping("/")
public class SimpleRestController {
    private static final ListeningExecutorService executor = MoreExecutors.listeningDecorator(Executors.newCachedThreadPool());
    
    @Autowired
    private RestTemplate restTemplate;

    @Autowired
    private Tracer tracer;

    @Autowired
    private SpanNamer spanNamer;

    @RequestMapping(method = RequestMethod.GET)
    public String get() throws ExecutionException, InterruptedException {
        Callable<ResponseEntity<String>> callable1 = new Callable<ResponseEntity<String>>() {
            @Override
            public ResponseEntity<String> call() throws Exception {
                return requestService("http://ms-service-1:8080/rest/");
            }
        };
        Callable<ResponseEntity<String>> traceCallable1 = new TraceCallable<>
                (tracer, spanNamer, callable1, "get_service_1");

        ListenableFuture<ResponseEntity<String>> future1 = executor.submit(traceCallable1);

        ...

        // wait for results
        ResponseEntity<String> service1Response = future1.get();
        
        ...
        
        return serviceResponse.getBody();
    }

    private ResponseEntity<String> requestService(String serviceUrl) {
        return restTemplate.exchange(serviceUrl, HttpMethod.GET, null, String.class);
    }

}
\end{lstlisting}

Een \texttt{Tracer} en een \texttt{SpanNamer} worden geïnjecteerd door Sleuth en een \texttt{RestTemplate} door Spring. Omdat Sleuth automatisch tracing informatie toevoegt aan een \texttt{RestTemplate}, moet dit niet verder geconfigureerd worden.

\section{Sampling strategieën en performantieverschillen}
\label{sec:sampling}

Er wordt onderzocht wat voor overhead het toevoegen van tracing informatie met zich meebrengt op de drie verschillende testopstellingen (zie \ref{sec:opzetten}). Omdat Sleuth gebaseerd is op Dapper, wordt bekeken hoe verschillende sampling strategieën de performantie beïnvloeden. Uit \autocite{Sigelman2010} blijkt immers dat verzameling van traces de grootste invloed kan hebben op vertragingen in het netwerk bij het maken van requests.

Met een gesampelde trace wordt bedoeld dat de trace gemarkeerd is om verzameld te worden. In dit onderzoek betekent dit dat Sleuth een gesampelde trace zal doorsturen naar de Zipkin server. Om de sampling te definiëren voor een bepaalde service dient een Java bean aangemaakt te worden. Sleuth zal deze Sampler automatisch opmerken en gebruiken. Om bijvoorbeeld alle requests te tracen, kan gebruik gemaakt worden van Sleuth's \texttt{AlwaysSampler}.

Om slechts een percentage van de requests te tracen, kan een eigen sampler gedefinieerd worden als volgt:

\begin{lstlisting}[language=Java, basicstyle=\ttfamily\small, caption=Sleuth percentage sampler]
@Bean
public Sampler percentageSampler() {
    return new Sampler() {
        @Override
        public boolean isSampled(Span span) {
            Random rg = new Random();
            // trace 50% of all requests
            return rg.nextInt(10) > 4;
        }
    };
}
\end{lstlisting}

Dit onderzoek gaat na welke performantiegevolgen verschillende sampling percentages op de verschillende gedefinieerde microservice opstellingen (zie \ref{sec:opzetten}) hebben. Er wordt gebruik gemaakt van de Apache Benchmark (ab) tool om de responstijd te testen. Bij de testen wordt rekening gehouden met een \textit{concurrency} variabele, dat aanduidt hoeveel requests er tegelijk worden uitgevoerd. Voor een hoog netwerkverkeer te simuleren worden er 1000 requests tegelijk uitgevoerd tot er 10 000 requests in totaal aangekomen zijn. De bovenlimiet 1000 is gekozen, omdat dit het maximaal aantal verbindingen is dat het gebruikte testsysteem tegelijk toelaat. Een gemiddeld netwerkverkeer wordt gesimuleerd door 200 requests tegelijk uit te voeren tot alweer 10 000 requests verwerkt zijn en een laag netwerkverkeer zal ten slotte 20 requests tegelijk uitvoeren.

\begin{tikzpicture}
\begin{axis}[
    title={Sampling performantie in hoge complexiteit setup},
    xlabel={Sampling frequentie},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1.1,
    ymin=0, ymax=100,
    xtick={0, 0.1, 0.5, 1},
    ytick={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(0.1,32.621)(0.5,32.769)(1,36.091)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(0.1,33.752)(0.5,43.422)(1,43.45)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(0.1,41.192)(0.5,78.317)(1,81.184)};

\addlegendentry{laag netwerkverkeer}
\addlegendentry{gemiddeld netwerkverkeer}
\addlegendentry{hoog netwerkverkeer}
\end{axis}
\end{tikzpicture}

Voor de hoge complexiteit setup blijkt dat tracing vanaf 50\% sampling frequentie en bij hoog netwerkverkeer een grote stijging kent in responstijd ten opzichte van de responstijd bij 10\% sampling. De responstijd stijgt van 41,2 ms naar 78,3 ms voor 50\% sampling en 81,2 ms voor 100\% sampling.

\begin{tikzpicture}
\begin{axis}[
    title={Sampling performantie in gemiddelde complexiteit setup},
    xlabel={Sampling frequentie},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1.1,
    ymin=0, ymax=20,
    xtick={0, 0.1, 0.5, 1},
    ytick={0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(0.1,12.482)(0.5,13.398)(1,15.514)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(0.1,13.109)(0.5,14.249)(1,14.948)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(0.1,13.158)(0.5,14.153)(1,16.325)};

\addlegendentry{laag netwerkverkeer}
\addlegendentry{gemiddeld netwerkverkeer}
\addlegendentry{hoog netwerkverkeer}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
    title={Sampling performantie in lage complexiteit setup},
    xlabel={Sampling frequentie},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1.1,
    ymin=0, ymax=20,
    xtick={0, 0.1, 0.5, 1},
    ytick={0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(0.1,4.225)(0.5,4.875)(1,4.08)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(0.1,4.685)(0.5,5.477)(1,4.802)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(0.1,8.535)(0.5,10.964)(1,12.428)};

\addlegendentry{laag netwerkverkeer}
\addlegendentry{gemiddeld netwerkverkeer}
\addlegendentry{hoog netwerkverkeer}
\end{axis}
\end{tikzpicture}

Voor de gemiddelde en lage complexiteit opstellingen heeft de sampling frequentie slechts een minimale impact op de totale performantie van het systeem.

\section{Logging}

Voor de logging worden er 4 extra microservices toegevoegd aan de testopstelling: ElasticSearch, Logstash, Fluentd en Kibana.

Om ervoor te zorgen dat de logs van ms-web en ms-service-n verzameld worden door Logstash of Fluentd wordt gebruik gemaakt van de Logback configuratie van Spring Boot, waar wordt aangegeven waar de Logstash of Fluentd server zich bevindt.

\begin{lstlisting}[language=Java, basicstyle=\ttfamily\scriptsize, caption=Voorbeeld Fluentd logback configuratie]
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml"/>
    <property name="FLUENTD_HOST" value="${FLUENTD_HOST:-192.168.99.100}"/>
    <property name="FLUENTD_PORT" value="${FLUENTD_PORT:-24224}"/>
    <appender name="FLUENT" class="ch.qos.logback.more.appenders.DataFluentAppender">
        <tag>dab</tag>
        <label>normal</label>
        <remoteHost>${FLUENTD_HOST}</remoteHost>
        <port>${FLUENTD_PORT}</port>
    </appender>

    <logger name="com.frederic" level="INFO" additivity="false">
        <appender-ref ref="CONSOLE" />
        <appender-ref ref="FILE" />
        <appender-ref ref="FLUENT" />
    </logger>

</configuration>
\end{lstlisting}

Voor Logstash dient de logstash-logback-encoder bibliotheek toegevoegd te worden aan elke microservice waarvan logging verzameld wilt worden. Voor Fluentd dienen de logback-more-appenders en fluent-logger bibliotheken toegevoegd te worden.

Er wordt onderzocht wat voor performantiegevolgen de verzameling van logs van de microservices naar ElasticSearch met zich meedraagt. De log shippers Logstash en Fluentd worden met elkaar vergeleken.  Er wordt hier ook gebruik gemaakt van de ab tool om de responstijd te testen.

\begin{tikzpicture}
\begin{axis}[
    title={Logging performantie in hoge complexiteit setup},
    xlabel={Concurrency (aantal verbindingen tegelijk)},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1000,
    ymin=0, ymax=100,
    xtick={20, 200, 1000},
    ytick={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(20,32.62)(200,33.75)(1000,41.19)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(20,46.22)(200,45.97)(1000,51.66)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(20,42.55)(200,44.1)(1000,50.37)};

\addlegendentry{zonder logging}
\addlegendentry{Logstash}
\addlegendentry{Fluentd}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
    title={Logging performantie in gemiddelde complexiteit setup},
    xlabel={Concurrency (aantal verbindingen tegelijk)},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1000,
    ymin=0, ymax=20,
    xtick={20, 200, 1000},
    ytick={0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(20,10.5)(200,12.36)(1000,14.87)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(20,12.43)(200,15.98)(1000,17.14)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(20,10.9)(200,13.14)(1000,15.42)};

\addlegendentry{zonder logging}
\addlegendentry{Logstash}
\addlegendentry{Fluentd}
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
    title={Logging performantie in lage complexiteit setup},
    xlabel={Concurrency (aantal verbindingen tegelijk)},
    ylabel={Gemiddelde responstijd (ms)},
    xmin=0, xmax=1000,
    ymin=0, ymax=20,
    xtick={20, 200, 1000},
    ytick={0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20},
    %xticklabel style={/pgf/number format/.cd,frac,frac TeX=\displaystyle\frac},
    legend pos=outer north east,
    ymajorgrids=true,
    grid style=dashed,
    xlabel near ticks,
    ylabel near ticks
]
 
\addplot[
    color=green,
    mark=square,
    ]
    coordinates {(20,4.18)(200,4.91)(1000,5.26)};
    
\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {(20,6.2)(200,7.8)(1000,8.63)};
    
\addplot[
    color=red,
    mark=square,
    ]
    coordinates {(20,5.9)(200,6.14)(1000,7.42)};

\addlegendentry{zonder logging}
\addlegendentry{Logstash}
\addlegendentry{Fluentd}
\end{axis}
\end{tikzpicture}

In alle complexiteitsopstellingen worden slechts kleine performantiegevolgen genoteerd door het toevoegen van logging aggregatie. Fluentd kent over alle opstellingen heen een iets betere performantie dan Logstash, maar het verschil blijft miniem. Beide log shippers kennen een goede performantie bij een laag tot hoog netwerkverkeer.

